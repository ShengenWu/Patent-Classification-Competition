{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"15XhwoP7kZLBCXgis-HgbPHKV4i0NoTzm","authorship_tag":"ABX9TyOBfy9SndAj1SaTNZ3Mj1Ug"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KVD3NMnPKWyp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662515299363,"user_tz":-480,"elapsed":8922,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}},"outputId":"ba4ca234-87a0-4a9f-ebd3-200588c4e3d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 9.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 70.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 54.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["# 导入transformers\n","import transformers\n","from transformers import AutoConfig,AutoModel,AutoTokenizer,logging\n","from torch.utils.data import RandomSampler,Dataset, DataLoader\n","from transformers import BertModel, BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup\n","from transformers import RobertaTokenizer, RobertaModel\n","# 导入torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# 常用包\n","import re\n","import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import pickle\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","from sklearn.utils import resample\n","from sklearn.metrics import accuracy_score\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"r8ay7IJQKxdY","executionInfo":{"status":"ok","timestamp":1662515309214,"user_tz":-480,"elapsed":7556,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["cache_dir = '/content/drive/MyDrive/Colab Notebooks/Academic-paper-classification/cache'\n","class PaperClassifier(nn.Module):\n","    def __init__(self):\n","        n_classes = 36\n","        super(PaperClassifier, self).__init__()\n","        PRE_TRAINED_MODEL_NAME = \"hfl/chinese-bert-wwm\"\n","        self.robert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,return_dict=False)\n","        self.bilstm = nn.LSTM(input_size=self.robert.config.hidden_size,\n","                              hidden_size=self.robert.config.hidden_size, batch_first=True, bidirectional=True)\n","        self.drop = nn.Dropout(p=0.3)\n","        self.out = nn.Linear(self.robert.config.hidden_size * 2, n_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","\n","        last_hidden_out, pooled_output = self.robert(  # 只要了句子级表示？    _:[10, 300, 768]    [16, 768]\n","            input_ids=input_ids,\n","            attention_mask=attention_mask  # [16, 300]300是句子长度\n","\n","        )\n","        last_hidden_out = self.drop(last_hidden_out)\n","        output_hidden, _ = self.bilstm(last_hidden_out)  # [10, 300, 768]\n","\n","        output = self.drop(output_hidden)  # dropout\n","        output = output.mean(dim=1)\n","\n","        return self.out(output)"],"metadata":{"id":"IYXvMwmfLD9P","executionInfo":{"status":"ok","timestamp":1662515959232,"user_tz":-480,"elapsed":373,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def data_process():\n","    train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Academic-paper-classification/data/train_clean_data.csv', sep='\\t')\n","    test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Academic-paper-classification/data/test_clean_data.csv', sep='\\t')\n","    return train, test"],"metadata":{"id":"cvkceNTvLfl5","executionInfo":{"status":"ok","timestamp":1662515411209,"user_tz":-480,"elapsed":383,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class PaperDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        \"\"\"\n","        item 为数据索引，迭代取第item条数据\n","        \"\"\"\n","        text = str(self.texts[item])\n","        label = self.labels[item]\n","\n","        encoding = self.tokenizer.encode_plus(  # 等价于tokenizer.tokenize() + tokenizer.convert_tokens_to_ids()\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            truncation=True,\n","            return_token_type_ids=True,\n","            pad_to_max_length=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'texts': text,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }"],"metadata":{"id":"hB3Iuo_oL8vz","executionInfo":{"status":"ok","timestamp":1662515373112,"user_tz":-480,"elapsed":439,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def create_data_loader(df, tokenizer, max_len, batch_size,sampler):\n","    ds = PaperDataset(  # dataset\n","        texts=df['text'].values,\n","        labels=df['label'].values,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        sampler = sampler,\n","        num_workers=4,  # 多线程\n","        pin_memory=True  # 页锁定内存\n","    )\n","\n","\n","def create_test_loader(df, tokenizer, max_len, batch_size):\n","    ds = PaperDataset(  # dataset\n","        texts=df['text'].values,\n","        labels=df['label'].values,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(  \n","        ds,\n","        batch_size=batch_size,\n","        num_workers=4,#多线程\n","        pin_memory=True,  # 页锁定内存\n","        shuffle=False\n","    )"],"metadata":{"id":"jd6inrLzNHt3","executionInfo":{"status":"ok","timestamp":1662515375551,"user_tz":-480,"elapsed":490,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n","    print(\"start training!\")\n","    model = model.train()\n","    losses = []\n","    pred_ls = []\n","    label_ls = []\n","    for d in tqdm(data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        targets = d[\"labels\"].to(device)\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","        _, preds = torch.max(outputs, dim=1)\n","        loss = loss_fn(outputs, targets)\n","        losses.append(loss.item())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        label_ls.extend(d[\"labels\"])\n","        pred_ls.extend(preds.tolist())\n","    correct_predictions = accuracy_score(label_ls, pred_ls)\n","    return correct_predictions, np.mean(losses)\n","\n","# 验证\n","def eval_model(model, data_loader, loss_fn, device):\n","    model = model.eval()  # 验证预测模式\n","    losses = []\n","    pred_ls = []\n","    label_ls = []\n","    with torch.no_grad():\n","        for d in data_loader:\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            targets = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","\n","            loss = loss_fn(outputs, targets)\n","            losses.append(loss.item())\n","            pred_ls.extend(preds.tolist())\n","            label_ls.extend(d[\"labels\"])\n","           \n","        correct_predictions = accuracy_score(label_ls, pred_ls)\n","    return correct_predictions, np.mean(losses)"],"metadata":{"id":"5RXDav5FNVz_","executionInfo":{"status":"ok","timestamp":1662515378362,"user_tz":-480,"elapsed":362,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def model_predictions(model, data_loader, device):\n","    model = model.eval()\n","    result = []\n","    with torch.no_grad():\n","        for d in data_loader:\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","            y_pred = outputs.data.cpu().numpy()\n","            result.extend(y_pred)\n","    \n","    return result"],"metadata":{"id":"RBYjcxGhNnh1","executionInfo":{"status":"ok","timestamp":1662515381499,"user_tz":-480,"elapsed":485,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def get_parameters(model, model_init_lr, multiplier, classifier_lr):\n","    parameters = []\n","    lr = model_init_lr\n","    for layer in range(12, -1, -1):  # 遍历模型的每一层\n","        layer_params = {\n","            'params': [p for n, p in model.named_parameters() if f'encoder.layer.{layer}.' in n],\n","            'lr': lr\n","        }\n","        parameters.append(layer_params)\n","        lr *= multiplier  # 每一层的学习率*0.95的衰减因子\n","    classifier_params = {\n","        'params': [p for n, p in model.named_parameters() if 'layer_norm' in n or 'linear' in n\n","                   or 'pooling' in n],\n","        'lr': classifier_lr  # 单独针对全连接层\n","    }\n","    parameters.append(classifier_params)\n","    return parameters"],"metadata":{"id":"LO6TPnO5NxJW","executionInfo":{"status":"ok","timestamp":1662515383805,"user_tz":-480,"elapsed":509,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from torch.utils import data\n","from sklearn.utils import resample\n","def load_data_kfold(dataset,BATCH_SIZE,MAX_LEN, k, n):\n","    print(\"第{}折正在划分数据集\".format(n+1))\n","\n","    l = len(dataset)\n","    print(l)\n","    shuffle_dataset = True\n","    random_seed = 42  # fixed random seed\n","    indices = list(range(l))\n","\n","    if shuffle_dataset:\n","        np.random.seed(random_seed)\n","        np.random.shuffle(indices)  # shuffle\n","    # Collect indexes of samples for validation set.\n","    val_indices = indices[int(l / k) * n:int(l / k) * (n + 1)]\n","    train_indices = list(set(indices).difference(set(val_indices)))\n","    train_sampler = data.SubsetRandomSampler(train_indices)  # build Sampler\n","    valid_sampler = data.SubsetRandomSampler(val_indices)\n","    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm')\n","    train_data_loader = create_data_loader(dataset, tokenizer, MAX_LEN, BATCH_SIZE,train_sampler)\n","    val_data_loader = create_data_loader(dataset, tokenizer, MAX_LEN, BATCH_SIZE,valid_sampler)\n","\n","    print(\"划分完成\")\n","    return train_data_loader, val_data_loader"],"metadata":{"id":"8yzgzqjXN26S","executionInfo":{"status":"ok","timestamp":1662515385943,"user_tz":-480,"elapsed":376,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#训练模型\n","def train_start(EPOCHS,MAX_LEN,BATCH_SIZE,train,test_data_loader):\n","    #模型定义\n","    model = PaperClassifier()\n","    model = model.to(device)\n","    #普通学习率\n","    k_fold = 5\n","    predict_all = np.zeros([20839,36])#存储测试集的 预测结果\n","    for n in range(k_fold):\n","        train_data_loader, val_data_loader = load_data_kfold(train, BATCH_SIZE,MAX_LEN, k_fold, n)\n","        #使用差分学习率\n","        parameters = get_parameters(model, 2e-5, 0.95, 1e-4)\n","        # 使用AdamW优化器\n","        optimizer = AdamW(parameters)\n","        total_steps = len(train_data_loader) * EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=0,\n","            num_training_steps=total_steps\n","        )\n","        loss_fn = nn.CrossEntropyLoss().to(device)\n","        best_accuracy = 0\n","        for epoch in range(EPOCHS):\n","            print(f'Epoch {epoch + 1}/{EPOCHS}')\n","            print('-' * 10)\n","            train_acc, train_loss = train_epoch(\n","                model,\n","                train_data_loader,\n","                loss_fn,\n","                optimizer,\n","                device,\n","                scheduler\n","            )\n","\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","            val_acc, val_loss = eval_model(\n","                model, val_data_loader, loss_fn, device)\n","            print(f'Val loss {val_loss} accuracy {val_acc}')\n","\n","            if val_acc > best_accuracy:\n","                torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/Academic-paper-classification/model/best_model_state_base_aug_15w.bin')\n","                best_accuracy = val_acc\n","\n","        #进行预测\n","        y_pred = model_predictions(model, test_data_loader, device)\n","        predict_all += np.array(y_pred)\n","    # 取每折的预测矩阵的平均\n","    predictions = predict_all/k_fold\n","    np.save(\"submit_arr.npy\", predictions)\n","    pred = np.argmax(predictions, axis=1)\n","    print(\"计算完成\")\n","    # 生成提交文件\n","    model_name = \"Bert_base_cross\"\n","    # 读取提交格式文件\n","    sub = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Academic-paper-classification/data/submit_example_A.csv')\n","    sub['label'] = list(pred)\n","    sub.to_csv('/content/drive/MyDrive/Colab Notebooks/Academic-paper-classification/data/submit/submit_{}.csv'.format(model_name), index=False)"],"metadata":{"id":"ba7mZ31zN9_S","executionInfo":{"status":"ok","timestamp":1662516301727,"user_tz":-480,"elapsed":387,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["train, test= data_process()\n","EPOCHS = 3 \n","MAX_LEN = 300 # 文本最大长度\n","BATCH_SIZE = 10\n","PRE_TRAINED_MODEL_NAME = 'hfl/chinese-bert-wwm' \n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","test_data_loader = create_test_loader(test, tokenizer, MAX_LEN, BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9vzx9UCOsYK","executionInfo":{"status":"ok","timestamp":1662516306375,"user_tz":-480,"elapsed":2015,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}},"outputId":"2d93dea5-ccd9-4825-d956-b2fb52fe6681"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["train_start(EPOCHS,MAX_LEN,BATCH_SIZE,train,test_data_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"id":"hlF8TSOMO0CJ","executionInfo":{"status":"error","timestamp":1662516839114,"user_tz":-480,"elapsed":368,"user":{"displayName":"Shengen Wu","userId":"08348379833208915743"}},"outputId":"37f579b2-fb52-4b41-e86b-59521f72ac28"},"execution_count":21,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-275672cde4bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-7ac9fc006a21>\u001b[0m in \u001b[0;36mtrain_start\u001b[0;34m(EPOCHS, MAX_LEN, BATCH_SIZE, train, test_data_loader)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#进行预测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mpredict_all\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# 取每折的预测矩阵的平均\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-de5062809eb2>\u001b[0m in \u001b[0;36mmodel_predictions\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m             )\n\u001b[1;32m     12\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}